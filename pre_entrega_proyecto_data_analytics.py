# -*- coding: utf-8 -*-
"""Pre-Entrega Proyecto Data Analytics

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ylZfo8BzVOOVH8liJxHsiOP_iub5S17Q

# „Éª‚òÜ„Éª **Pre-Entrega Proyecto Final - Data Analytics con Python** „Éª‚òÜ„Éª

---

‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£

### **¬°Hola! Soy Melany Leguizam√≥n, y este es mi proyecto para el curso "Data Analytics con Python" de Talento Tech. üë©‚Äçüíª**

*Es mi primera vez programando con pandas. El c√≥digo est√° sujeto a correcciones futuras.*

‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£‚å£

‚ïì‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚òÜ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïñ
  

> **Tabla de contenidos:**

  1. **Importaci√≥n de m√≥dulos y carga de datos:**

*   Cargamos datasets e importamos bibliotecas.

  2. **Exploraci√≥n de datos EDA:**

*   *Exploratory Data Analysis* de los dataframes.

  3. **An√°lisis de calidad de datos:**

*   An√°lisis de valores nulos y duplicados.

  4. **Limpieza de datasets:**

*   Creamos copias de los datasets y trabajamos sobre ellas.

  5. **Transformaci√≥n de datos:**

*   Analizamos los productos de alto rendimiento.

  6. **Agregaci√≥n:**

*   Res√∫men de ventas por categor√≠a.


‚ïô‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚òÜ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïú

### **¬°Empecemos! (‚óè'‚ó°'‚óè):**

### 1. Importaci√≥n de m√≥dulos y carga de datos.
"""

# Importamos bibliotecas necesarias.
!pip install colorama
from colorama import Fore, Back, Style, init # Uso colorama para darle estilo al texto :)
import pandas as pd
import numpy as np

# Montamos la unidad
from google.colab import drive
drive.mount('/content/drive')

# Verificamos que los datasets se encuentren en el drive.
import os
os.listdir("/content/drive/MyDrive/datasets")

# Definimos las rutas.
ruta_ventas = "/content/drive/MyDrive/datasets/ventas.csv"
ruta_clientes = "/content/drive/MyDrive/datasets/clientes.csv"
ruta_marketing = "/content/drive/MyDrive/datasets/marketing.csv"

# Cargamos los archivos como datasets.
ventas = pd.read_csv(ruta_ventas)
clientes = pd.read_csv(ruta_clientes)
marketing = pd.read_csv(ruta_marketing)

# Chequeamos que los datos se hayan cargado correctamente.
print(Fore.CYAN + Style.BRIGHT + "DATOS VENTAS", ventas.shape)
print(Fore.GREEN + Style.BRIGHT +"DATOS CLIENTES", clientes.shape)
print(Fore.MAGENTA + Style.BRIGHT +"DATOS MARKETING", marketing.shape)

display(ventas.head())
display(clientes.head())
display(marketing.head())

"""### 2. Exploraci√≥n de datos EDA."""

def eda(df, nombre):
    print(Fore.BLUE + Style.BRIGHT + f"=== An√°lisis de Datos: {nombre} ===")
    print(Style.RESET_ALL)
    print(Fore.BLUE + "Dimensiones del dataframe:")
    print(Style.RESET_ALL + f"  Filas: {df.shape[0]}, Columnas: {df.shape[1]}")
    print(Fore.BLUE + "\nColumnas:")
    print(Style.RESET_ALL + f"  {list(df.columns)}")
    print(Fore.BLUE + "\nTipos de Datos:")
    print(Style.RESET_ALL)
    print(df.dtypes)
    print(Fore.BLUE + "\nValores Nulos por Columna:")
    print(Style.RESET_ALL)
    print(df.isna().sum())
    print(Fore.BLUE + "\nPrimeras 5 Filas:")
    print(Style.RESET_ALL)

    display(df.head(5))
    print(Fore.BLUE + "\nDescribe (num√©rico):")
    print(Style.RESET_ALL)
    display(df.describe(include='number'))
    print(Fore.BLUE + "\nDescribe (categ√≥rico):")
    print(Style.RESET_ALL)
    display(df.describe(include='object'))
    print(Fore.BLUE + "\n" + "="*100)
    print(Style.RESET_ALL)

eda(ventas, "VENTAS (inicial)")

eda(clientes, "CLIENTES (inicial)")

eda(marketing, "MARKETING (inicial)")

"""### 3. An√°lisis de calidad de datos."""

# Analizamos la cantidad de valores duplicados y nulos que hay por dataframe.
print(Fore.BLUE + Style.BRIGHT + "=== Res√∫men calidad de datos ===")
print(Style.RESET_ALL)

print(Fore.BLUE + "\nValores nulos por dataframe:")
print(Style.RESET_ALL)

def calidad_nulos(df, name):         # Funci√≥n para buscar valores nulos.
    print(f"\n{name}:")
    null_counts = df.isnull().sum()
    if null_counts.sum() == 0:
        print(Fore.GREEN + "  No hay valores nulos." + Style.RESET_ALL)
    else:
        for column, count in null_counts.items():
            if count > 0:
                print(Fore.RED + f"  {column}: {count}" + Style.RESET_ALL)    # Resalta en rojo si encuentra nulos.
            else:
                print(f"  {column}: {count}")

calidad_nulos(ventas, "Ventas")
calidad_nulos(clientes, "Clientes")
calidad_nulos(marketing, "Marketing")

print(Fore.BLUE + "\nValores duplicados por dataframe:")
print(Style.RESET_ALL)

def calidad_duplicados(df, name):         # Funci√≥n para buscar valores duplicados.
    duplicate_count = df.duplicated().sum()
    if duplicate_count > 0:
        print(f"{name}: " + Fore.RED + str(duplicate_count) + Style.RESET_ALL)
    else:
        print(f"{name}: 0")

calidad_duplicados(ventas, "Ventas")
calidad_duplicados(clientes, "Clientes")
calidad_duplicados(marketing, "Marketing")

# Como encontramos nulos y duplicados en ventas, creamos una funci√≥n que muestre detalles.
def display_calidad(df, name):
    print(Fore.BLUE + Style.BRIGHT + f"=== Valores nulos y duplicados - {name} ===")
    print(Style.RESET_ALL)

    print(Fore.BLUE + f"\nFilas con nulos:" + Style.RESET_ALL)
    filas_nulos = df[df.isnull().any(axis=1)]                   # Variable para valores nulos.
    if filas_nulos.empty:
        print(f"  No hay filas con valores nulos en {name}.")
    else:
        display(filas_nulos)

    print(Fore.BLUE + f"\nFilas duplicadas:" + Style.RESET_ALL)
    filas_duplicadas = df[df.duplicated(keep=False)]            # Variable para valores duplicados.
    if filas_duplicadas.empty:
        print(f"  No hay filas duplicadas en {name}.")
    else:
        cantidad_duplicados = filas_duplicadas.groupby(list(filas_duplicadas.columns)).size().reset_index(name='cantidad_duplicados') # Crea una columna con la cantidad de veces que se repite una fila.
        filtro_duplicados = pd.merge(filas_duplicadas, cantidad_duplicados, on=list(filas_duplicadas.columns)).sort_values(by='cantidad_duplicados', ascending=False) # Filtro para que muestre primero los valores que m√°s se repiten.
        display(filtro_duplicados)

display_calidad(ventas, "Ventas")

# Testeamos que funcione bien con dem√°s dataframes.
display_calidad(clientes, "Clientes")
print("\n")
display_calidad(marketing, "Marketing")

"""### 4. Limpieza de datasets."""

# Creamos copias de los datasets para no modificar los originales.
ventas_clean = ventas.copy()
clientes_clean = clientes.copy()
marketing_clean = marketing.copy()

# Eliminamos filas duplicadas.
ventas_clean = ventas_clean.drop_duplicates()
clientes_clean = clientes_clean.drop_duplicates()
marketing_clean = marketing_clean.drop_duplicates()

# Comprobamos que funciona.
calidad_duplicados(ventas_clean, "Ventas")

# Normalizamos las fechas a formato datetime.
for df in [ventas_clean, clientes_clean, marketing_clean]:
    for col in df.columns:
        if "fecha" in col.lower():
            df[col] = pd.to_datetime(df[col], errors="coerce", dayfirst=True)

# Comprobamos que funciona.
print(Fore.BLUE + Style.BRIGHT + "Tipos de datos ventas_clean:\n" + Style.RESET_ALL, ventas_clean.dtypes, "\n")
print(Fore.BLUE + Style.BRIGHT + "Tipos de datos clientes_clean:\n" + Style.RESET_ALL, clientes_clean.dtypes, "\n")
print(Fore.BLUE + Style.BRIGHT + "Tipos de datos marketing_clean:\n" + Style.RESET_ALL, marketing_clean.dtypes, "\n")

# Creamos una funci√≥n para normalizar el texto.
def normalizar_texto(df):
    for col in df.select_dtypes(include="object").columns:
        df[col] = (
            df[col]
            .astype(str)    # Convierte todo a texto.
            .str.strip()    # Elimina espacios innecesarios.
            .str.replace(r"[\u200b\t\r\n]", "", regex=True)   # Elimina patrones invisibles.
            .str.replace(" +", " ", regex=True)   # Reemplaza espacios consecutivos por uno solo.
            .str.title()    # Reemplaza por may√∫scula la primera letra de cada palabra.
            )
        return df

# Normalizamos el texto.
ventas_clean = normalizar_texto(ventas_clean)
clientes_clean = normalizar_texto(clientes_clean)
marketing_clean = normalizar_texto(marketing_clean)

# Chequeamos que funciona.
print(Fore.BLUE + Style.BRIGHT + "Texto normalizado ventas_clean:\n" + Style.RESET_ALL, ventas_clean.head(10), "\n")
print(Fore.BLUE + Style.BRIGHT + "Texto normalizado clientes_clean:\n"+ Style.RESET_ALL, clientes_clean.head(10), "\n")
print(Fore.BLUE + Style.BRIGHT + "Texto normalizado marketing_clean:\n" + Style.RESET_ALL, marketing_clean.head(10), "\n")

# Normalizamos el precio de ventas para que coincida con dem√°s datasets.
if "precio" in ventas_clean.columns:
  ventas_clean["precio"] = (
        ventas_clean["precio"]
        .astype(str)
        .str.replace("$", "", regex=False)  # Eliminamos s√≠mbolo "$".
        .str.replace(",", "", regex=False)  # Eliminamos la coma de los miles.
        .str.strip()  # Eliminamos espacios innecesarios.
        )
  ventas_clean["precio"] = pd.to_numeric(ventas_clean["precio"], errors="coerce")

# Convertimos el tipo de dato de "cantidad" a Int64 para permiir valores nulos.
if "cantidad" in ventas_clean.columns:
    ventas_clean["cantidad"] = pd.to_numeric(
        ventas_clean["cantidad"], errors="coerce"
    ).astype("Int64")

# Chequeamos que haya funcionado.
print(Fore.BLUE + Style.BRIGHT + "Tipos de datos ventas_clean:\n" + Style.RESET_ALL, ventas_clean.dtypes, "\n")

# Guardamos los datasets limpios
ventas_clean.info()
ventas_clean.to_csv("/content/drive/MyDrive/datasets/ventas_clean.csv", index=False)
clientes_clean.to_csv("/content/drive/MyDrive/datasets/clientes_clean.csv", index=False)
marketing_clean.to_csv("/content/drive/MyDrive/datasets/marketing_clean.csv", index=False)

print(Fore.GREEN + "Archivos ventas_clean.csv, clientes_clean.csv, marketing_clean.csv guardados con √©xito" + Style.RESET_ALL)

"""### 5. Transformaci√≥n de datos."""

# Calculamos el total de las ventas.
ventas_clean['total_venta'] = ventas_clean['precio'] * ventas_clean['cantidad']
ventas_por_producto = ventas_clean.groupby('producto')['total_venta'].sum().reset_index()

# Definimos un umbral (80%) para que la tabla nos muestre los productos cuyo ingreso sea 20% superior.
umbral = ventas_por_producto['total_venta'].quantile(0.80)
productos_alto_rendimiento = ventas_por_producto[ventas_por_producto['total_venta'] > umbral]

# Mostramos la tabla filtrada de mayor a menor ingreso.
print(Fore.BLUE + Style.BRIGHT + "Productos de alto rendimiento (Top 20% por ingreso):\n" + Style.RESET_ALL)
display(productos_alto_rendimiento.sort_values(by='total_venta', ascending=False))

"""### 6. Agregaci√≥n."""

# Agrupamos por categor√≠a y creamos las columnas.
ventas_por_categoria = ventas_clean.groupby('categoria').agg(
    ventas_totales=('total_venta', 'sum'),  # Sumamos las ventas para columna "ventas_totales".
    unidades=('cantidad', 'sum'),  # Sumamos las cantidades para columna "unidades".
    cantidad_ventas=('id_venta', 'count')  # Contamos la cantidad de ventas para columna "cantidad_ventas".
).reset_index()

# Calculamos ticket promedio (ingresos totales/conteo de ventas por categor√≠a).
ventas_por_categoria['ticket_promedio'] = ventas_por_categoria['ventas_totales'] / ventas_por_categoria['cantidad_ventas']

# Filtramos ingresos de mayor a menor.
ventas_por_categoria = ventas_por_categoria.sort_values(by='ventas_totales', ascending=False)

# Mostramos la tabla final.
print(Fore.BLUE + Style.BRIGHT + "Resumen de ventas por categor√≠a:\n" + Style.RESET_ALL)
display(ventas_por_categoria)